{
    "id": "cs-gpt-001",
    "title": "Generative Pre-trained Transformer",
    "profile": "stem.v1",
    "policy": {
        "sections": [
            {
                "id": "main",
                "title": "Generative Pre-trained Transformer Exam",
                "modules": [
                    {
                        "id": "main-m1",
                        "time_limit_sec": 3600
                    }
                ]
            }
        ],
        "navigation": {
            "allow_back": false,
            "module_locked": true
        },
        "item_constraints": {
            "main": {
                "mcq_single": {
                    "choices": 4
                },
                "mcq_multi": {
                    "choices": 5
                },
                "numeric": {
                    "allow_fraction": true,
                    "tolerance": 0.01
                }
            }
        }
    },
    "questions": [
        {
            "id": "q01",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "true_false",
            "prompt_html": "<p>In an auto-regressive Transformer language model, tokens are predicted conditioned on <em>past</em> tokens only (with a causal mask).</p><p>True or false?</p>",
            "answer_key": [
                "true"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Autoregression & Causal Masking"
            }
        },
        {
            "id": "q02",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "short_word",
            "prompt_html": "<p>What is the usual subword tokenization algorithm used in many GPT-style models that merges frequent symbol pairs?</p>",
            "answer_key": [
                "bpe"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Tokenization (BPE)"
            }
        },
        {
            "id": "q03",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>Which objective is used during pretraining of GPT-style models?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Masked language modeling (bidirectional)"
                },
                {
                    "id": "B",
                    "label_html": "Next sentence prediction"
                },
                {
                    "id": "C",
                    "label_html": "Causal language modeling (next-token prediction)"
                },
                {
                    "id": "D",
                    "label_html": "Contrastive image-text alignment"
                }
            ],
            "answer_key": [
                "C"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Training Objectives"
            }
        },
        {
            "id": "q04",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "numeric",
            "prompt_html": "<p>A single Transformer layer uses model width <em>d</em>=768 and 12 attention heads. What is the per-head key/query dimension if they evenly split the model width?</p><p>Enter an integer.</p>",
            "answer_key": [
                "64"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Multi-Head Attention Dimensions"
            }
        },
        {
            "id": "q05",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>Which component primarily mixes information <em>across</em> sequence positions in a Transformer block?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Feed-forward network (MLP)"
                },
                {
                    "id": "B",
                    "label_html": "Layer normalization"
                },
                {
                    "id": "C",
                    "label_html": "Self-attention"
                },
                {
                    "id": "D",
                    "label_html": "Dropout"
                }
            ],
            "answer_key": [
                "C"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Attention Mechanism"
            }
        },
        {
            "id": "q06",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_multi",
            "prompt_html": "<p>Which techniques are commonly used to stabilize and speed up Transformer training? Select <em>all</em> that apply.</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Learning rate warmup"
                },
                {
                    "id": "B",
                    "label_html": "Gradient clipping"
                },
                {
                    "id": "C",
                    "label_html": "Layer normalization"
                },
                {
                    "id": "D",
                    "label_html": "Residual connections"
                },
                {
                    "id": "E",
                    "label_html": "Removing positional information"
                }
            ],
            "answer_key": [
                "A",
                "B",
                "C",
                "D"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Optimization & Stabilization"
            }
        },
        {
            "id": "q07",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "true_false",
            "prompt_html": "<p>Rotary positional embeddings (RoPE) inject position information by rotating query/key vectors in a complex plane or 2D subspace.</p><p>True or false?</p>",
            "answer_key": [
                "true"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Positional Encoding (RoPE)"
            }
        },
        {
            "id": "q08",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>What does the key-value cache (KV cache) reduce during long-context inference?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Memory footprint of embeddings"
                },
                {
                    "id": "B",
                    "label_html": "Recomputation of past attention projections"
                },
                {
                    "id": "C",
                    "label_html": "Number of parameters on disk"
                },
                {
                    "id": "D",
                    "label_html": "Tokenizer vocabulary size"
                }
            ],
            "answer_key": [
                "B"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Inference Optimization"
            }
        },
        {
            "id": "q09",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "short_word",
            "prompt_html": "<p>Name the normalization technique commonly placed before sublayers in GPT variants (\"Pre-<em>something</em>\").</p>",
            "answer_key": [
                "pre-ln"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Architecture (Pre-LN)"
            }
        },
        {
            "id": "q10",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_multi",
            "prompt_html": "<p>Which sampling strategies are commonly used at inference time for text generation? Select all that apply.</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Greedy decoding"
                },
                {
                    "id": "B",
                    "label_html": "Beam search"
                },
                {
                    "id": "C",
                    "label_html": "Top-k sampling"
                },
                {
                    "id": "D",
                    "label_html": "Nucleus (top-p) sampling"
                },
                {
                    "id": "E",
                    "label_html": "Gradient descent sampling"
                }
            ],
            "answer_key": [
                "A",
                "B",
                "C",
                "D"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Decoding Strategies"
            }
        },
        {
            "id": "q11",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "numeric",
            "prompt_html": "<p>A model with a 50,000-token vocabulary and embedding size 1024 has how many embedding parameters? Ignore bias.</p><p>Enter an integer.</p>",
            "answer_key": [
                "51200000"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Parameter Counting"
            }
        },
        {
            "id": "q12",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>Which loss function is typically minimized during GPT pretraining?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Mean squared error between logits and tokens"
                },
                {
                    "id": "B",
                    "label_html": "Cross-entropy between predicted distribution and next token"
                },
                {
                    "id": "C",
                    "label_html": "Hinge loss on token ranks"
                },
                {
                    "id": "D",
                    "label_html": "Triplet loss on token embeddings"
                }
            ],
            "answer_key": [
                "B"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Loss Functions"
            }
        },
        {
            "id": "q13",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>Which component <em>does not</em> contain learned parameters in a standard attention head?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Softmax operator"
                },
                {
                    "id": "B",
                    "label_html": "Query projection matrix"
                },
                {
                    "id": "C",
                    "label_html": "Key projection matrix"
                },
                {
                    "id": "D",
                    "label_html": "Value projection matrix"
                }
            ],
            "answer_key": [
                "A"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Attention Internals"
            }
        },
        {
            "id": "q14",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "true_false",
            "prompt_html": "<p>Label smoothing is sometimes used in language modeling to improve calibration by distributing a small probability mass to non-target tokens.</p><p>True or false?</p>",
            "answer_key": [
                "true"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Regularization & Calibration"
            }
        },
        {
            "id": "q15",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>Which method enables efficient fine-tuning by injecting low-rank adapters into weight matrices?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Distillation"
                },
                {
                    "id": "B",
                    "label_html": "LoRA"
                },
                {
                    "id": "C",
                    "label_html": "DropConnect"
                },
                {
                    "id": "D",
                    "label_html": "Spectral normalization"
                }
            ],
            "answer_key": [
                "B"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Parameter-Efficient Tuning"
            }
        },
        {
            "id": "q16",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "short_word",
            "prompt_html": "<p>Fill in the blank: During RLHF, a model is often optimized with policy gradients against a learned <em>_______</em> model.</p>",
            "answer_key": [
                "reward"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "RLHF Basics"
            }
        },
        {
            "id": "q17",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_multi",
            "prompt_html": "<p>Which are practical benefits of quantization for LLMs? Select all that apply.</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Reduced memory footprint"
                },
                {
                    "id": "B",
                    "label_html": "Faster inference on some hardware"
                },
                {
                    "id": "C",
                    "label_html": "Guaranteed accuracy improvement"
                },
                {
                    "id": "D",
                    "label_html": "Potential throughput gains due to cache fit"
                },
                {
                    "id": "E",
                    "label_html": "Unlimited context length"
                }
            ],
            "answer_key": [
                "A",
                "B",
                "D"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Model Compression"
            }
        },
        {
            "id": "q18",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "numeric",
            "prompt_html": "<p>Given a model with 24 layers, each with an MLP hidden size of 3072 and input size 1024, estimate parameters in all MLP <em>input</em> projections combined: 24 × (1024 × 3072). Enter the integer result.</p>",
            "answer_key": [
                "75497472"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Parameter Estimation"
            }
        },
        {
            "id": "q19",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>What is the role of the residual connection around each sublayer?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Reduces sequence length"
                },
                {
                    "id": "B",
                    "label_html": "Improves gradient flow and training stability"
                },
                {
                    "id": "C",
                    "label_html": "Performs positional encoding"
                },
                {
                    "id": "D",
                    "label_html": "Applies dropout to inputs"
                }
            ],
            "answer_key": [
                "B"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Architecture (Residuals)"
            }
        },
        {
            "id": "q20",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>Which position encoding approach naturally supports extrapolation to longer contexts by phase rotation rather than absolute indexing?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Absolute learned embeddings"
                },
                {
                    "id": "B",
                    "label_html": "Sinusoidal absolute embeddings"
                },
                {
                    "id": "C",
                    "label_html": "Rotary positional embeddings (RoPE)"
                },
                {
                    "id": "D",
                    "label_html": "No positional information"
                }
            ],
            "answer_key": [
                "C"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Positional Encoding"
            }
        },
        {
            "id": "q21",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>Which diagram best represents a minimal Transformer block used in GPT inference?</p><svg xmlns='http: //www.w3.org/2000/svg' width='360' height='140' viewBox='0 0 360 140'><rect x='10' y='20' width='70' height='30' fill='#f0f8ff' stroke='#333'/><text x='20' y='40' font-size='12'>Input</text><rect x='100' y='20' width='90' height='30' fill='#e8ffe8' stroke='#333'/><text x='108' y='40' font-size='12'>Self-Attn</text><rect x='210' y='20' width='60' height='30' fill='#fff4e6' stroke='#333'/><text x='220' y='40' font-size='12'>Add</text><rect x='280' y='20' width='60' height='30' fill='#ffe8f0' stroke='#333'/><text x='290' y='40' font-size='12'>LN</text><rect x='100' y='80' width='90' height='30' fill='#e8ffe8' stroke='#333'/><text x='120' y='100' font-size='12'>MLP</text><rect x='210' y='80' width='60' height='30' fill='#fff4e6' stroke='#333'/><text x='220' y='100' font-size='12'>Add</text><rect x='280' y='80' width='60' height='30' fill='#ffe8f0' stroke='#333'/><text x='290' y='100' font-size='12'>LN</text><line x1='80' y1='35' x2='100' y2='35' stroke='#333'/><line x1='170' y1='35' x2='210' y2='35' stroke='#333'/><line x1='270' y1='35' x2='280' y2='35' stroke='#333'/><line x1='80' y1='95' x2='100' y2='95' stroke='#333'/><line x1='190' y1='95' x2='210' y2='95' stroke='#333'/><line x1='270' y1='95' x2='280' y2='95' stroke='#333'/></svg><p>Choose the best description.</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Input → MLP → LN → Add → Self-Attn → LN"
                },
                {
                    "id": "B",
                    "label_html": "Input → Self-Attn → Add → LN → MLP → Add → LN"
                },
                {
                    "id": "C",
                    "label_html": "Input → LN → Self-Attn → MLP → Add"
                },
                {
                    "id": "D",
                    "label_html": "Input → Add → Add → LN → MLP"
                }
            ],
            "answer_key": [
                "B"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Block Diagram (SVG)"
            }
        },
        {
            "id": "q22",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "true_false",
            "prompt_html": "<p>Increasing batch size during training always reduces the generalization gap and improves model quality.</p><p>True or false?</p>",
            "answer_key": [
                "false"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Generalization"
            }
        },
        {
            "id": "q23",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>Which term refers to the residual stream vector dimensionality in a Transformer?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Context window"
                },
                {
                    "id": "B",
                    "label_html": "Model width (d_model)"
                },
                {
                    "id": "C",
                    "label_html": "Number of layers"
                },
                {
                    "id": "D",
                    "label_html": "Number of experts"
                }
            ],
            "answer_key": [
                "B"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Dimensions & Notation"
            }
        },
        {
            "id": "q24",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>Which is a common mitigation for exposure bias in sequence generation?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Scheduled sampling (mixing ground-truth and model tokens)"
                },
                {
                    "id": "B",
                    "label_html": "Removing teacher forcing entirely"
                },
                {
                    "id": "C",
                    "label_html": "Always greedy decoding"
                },
                {
                    "id": "D",
                    "label_html": "Disable dropout"
                }
            ],
            "answer_key": [
                "A"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Training Pitfalls"
            }
        },
        {
            "id": "q25",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "short_word",
            "prompt_html": "<p>What is the name of the per-token scaling factor applied to QK<sup>T</sup> before softmax in attention?</p>",
            "answer_key": [
                "1/sqrt(d)"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Scaled Dot-Product Attention"
            }
        },
        {
            "id": "q26",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>Which choice best describes nucleus (top-p) sampling?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Always choose the argmax token"
                },
                {
                    "id": "B",
                    "label_html": "Sample from the top-k tokens by probability"
                },
                {
                    "id": "C",
                    "label_html": "Sample from the smallest token set whose probabilities sum to p"
                },
                {
                    "id": "D",
                    "label_html": "Beam search with width p"
                }
            ],
            "answer_key": [
                "C"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Decoding (Top-p)"
            }
        },
        {
            "id": "q27",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "essay",
            "prompt_html": "<p><strong>Short essay:</strong> Describe the trade-offs between greedy decoding, beam search, and nucleus sampling for open-ended generation. Include when you might prefer each.</p>",
            "answer_key": [],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Analysis & Communication"
            }
        },
        {
            "id": "q28",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "essay",
            "prompt_html": "<p><strong>Short essay:</strong> Explain how RLHF augments pretraining. Outline the stages (pretraining, supervised fine-tuning, reward modeling, policy optimization) and discuss expected effects on alignment and style.</p>",
            "answer_key": [],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "RLHF Concepts"
            }
        },
        {
            "id": "q29",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "essay",
            "prompt_html": "<p><strong>Short essay:</strong> Compare absolute positional embeddings, sinusoidal embeddings, and rotary positional embeddings with respect to extrapolation and inductive biases.</p>",
            "answer_key": [],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Position Encoding Comparison"
            }
        },
        {
            "id": "q30",
            "section_id": "main",
            "module_id": "main-m1",
            "type": "mcq_single",
            "prompt_html": "<p>Which practice best helps prevent data contamination when evaluating GPT models?</p>",
            "choices": [
                {
                    "id": "A",
                    "label_html": "Using any public dataset without checks"
                },
                {
                    "id": "B",
                    "label_html": "Ensuring evaluation sets are held-out and audited against pretraining corpora"
                },
                {
                    "id": "C",
                    "label_html": "Only reporting perplexity"
                },
                {
                    "id": "D",
                    "label_html": "Training longer on the evaluation set"
                }
            ],
            "answer_key": [
                "B"
            ],
            "points": 1,
            "tags": {
                "domain": "Generative Pre-trained Transformer",
                "skill": "Evaluation & Contamination"
            }
        }
    ]
}